{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d030e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3947adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5e034a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2850, 60, 60, 3)\n",
      "(2850, 3)\n",
      "(150, 60, 60, 3)\n",
      "(150, 3)\n"
     ]
    }
   ],
   "source": [
    "pickle_in = open(\"./data/train_x.pickle\",\"rb\")\n",
    "train_x = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"./data/train_y.pickle\",\"rb\")\n",
    "train_y = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"./data/test_x.pickle\",\"rb\")\n",
    "test_x = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"./data/test_y.pickle\",\"rb\")\n",
    "test_y = pickle.load(pickle_in)\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65048a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "n_input = 10800\n",
    "learning_rate = 0.001\n",
    "training_iters = 10\n",
    "batch_size = 512\n",
    "display_step = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed4a6efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of placeholder (?, 60, 60, 3) (?, 3)\n"
     ]
    }
   ],
   "source": [
    "img_size=60\n",
    "num_channels=3\n",
    "x = tf.compat.v1.placeholder(tf.float32, shape=[None, img_size,img_size,num_channels]) \n",
    "y_ = tf.compat.v1.placeholder(tf.float32, [None, num_classes])\n",
    "print('Shape of placeholder',x.shape, y_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "223a4b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool2d(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02c9083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'w1': tf.Variable(tf.random.normal([5, 5, 3, 32]),name='w1'),\n",
    "    'w2': tf.Variable(tf.random.normal([5, 5, 32, 64]),name='w2'),\n",
    "    'w3': tf.Variable(tf.random.normal([5, 5, 64, 128]),name='w3'),\n",
    "    'wd1': tf.Variable(tf.random.normal([8 * 8 * 128, 2048]),name='wd1'),  \n",
    "    'wout': tf.Variable(tf.random.normal([2048, num_classes]),name='wout')\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random.normal([32]),name='b1'),\n",
    "    'b2': tf.Variable(tf.random.normal([64]),name='b2'),\n",
    "    'b3': tf.Variable(tf.random.normal([128]),name='b3'),\n",
    "    'bd1': tf.Variable(tf.random.normal([2048]),name='bd1'),\n",
    "    'bout': tf.Variable(tf.random.normal([num_classes]),name='bout')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be02cc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_net(x, weights, biases):\n",
    "        \n",
    "    # reshape input to 60x60x3 size\n",
    "    x = tf.reshape(x, shape=[-1, 60, 60, 3])  \n",
    "    \n",
    "    print(\"----------------------------------------------------------------------------\")\n",
    "    print(\"size of x is\")\n",
    "    print(x.shape)\n",
    "    \n",
    "  \n",
    "    conv1 = conv2d(x, weights['w1'], biases['b1'])\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "    print(\"----------------------------------------------------------------------------\")\n",
    "    print(\"size after 1st conv layer is \")\n",
    "    print(conv1.shape)\n",
    "\n",
    "    \n",
    "    #input is 30*30*32 image\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['w2'], biases['b2'])\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    print(\"----------------------------------------------------------------------------\")\n",
    "    print(\"size after 2nd conv and pooling layer is\")\n",
    "    print(conv2.shape)\n",
    "    \n",
    "    \n",
    "    ### third conv layer\n",
    "    # input is 15*15*64 image\n",
    "    # Convolution Layer\n",
    "    conv3 = conv2d(conv2, weights['w3'], biases['b3'])\n",
    "  \n",
    "    conv3 = maxpool2d(conv3, k=2)\n",
    "    print(\"----------------------------------------------------------------------------\")\n",
    "    print(\"size after 3rd conv and pooling layer is\")\n",
    "    print(conv3.shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #input is 8*8*128 \n",
    "    # Fully connected layer\n",
    "    # Reshape conv3 output to fit fully connected layer input   = 8*8*128 = 8192\n",
    "    fc1 = tf.reshape(conv3, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    print(\"----------------------------------------------------------------------------\")\n",
    "    print(\"shape after flattening the image\")\n",
    "    print(fc1)  #8192 is the output\n",
    "    \n",
    "    \n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    print(\"----------------------------------------------------------------------------\")\n",
    "    print(\"shape after fully connected layer\")\n",
    "    print(fc1)\n",
    "    \n",
    "    \n",
    "    # Output, class prediction\n",
    "    # finally we multiply the fully connected layer with the weights and add a bias term. \n",
    "    out = tf.add(tf.matmul(fc1, weights['wout']), biases['bout'])\n",
    "    print(\"----------------------------------------------------------------------------\")\n",
    "    print(\"Output layer\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "146f8b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "size of x is\n",
      "(?, 60, 60, 3)\n",
      "----------------------------------------------------------------------------\n",
      "size after 1st conv layer is \n",
      "(?, 30, 30, 32)\n",
      "----------------------------------------------------------------------------\n",
      "size after 2nd conv and pooling layer is\n",
      "(?, 15, 15, 64)\n",
      "----------------------------------------------------------------------------\n",
      "size after 3rd conv and pooling layer is\n",
      "(?, 8, 8, 128)\n",
      "----------------------------------------------------------------------------\n",
      "shape after flattening the image\n",
      "Tensor(\"Reshape_1:0\", shape=(?, 8192), dtype=float32)\n",
      "----------------------------------------------------------------------------\n",
      "shape after fully connected layer\n",
      "Tensor(\"Relu_3:0\", shape=(?, 2048), dtype=float32)\n",
      "----------------------------------------------------------------------------\n",
      "Output layer\n",
      "Tensor(\"Add_1:0\", shape=(?, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model = conv_net(x, weights, biases)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd9bea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, labels=y_))\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bede4e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.compat.v1.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f9a2a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  0  -  cost:  3238874.2\n",
      "epoch :  1  -  cost:  5154295.0\n",
      "epoch :  2  -  cost:  5620969.5\n",
      "epoch :  3  -  cost:  4324294.0\n",
      "epoch :  4  -  cost:  2093509.1\n",
      "epoch :  5  -  cost:  2098013.0\n",
      "epoch :  6  -  cost:  3128101.5\n",
      "epoch :  7  -  cost:  2276721.0\n",
      "epoch :  8  -  cost:  1269229.6\n",
      "epoch :  9  -  cost:  1463815.1\n",
      "epoch :  10  -  cost:  2042287.9\n",
      "epoch :  11  -  cost:  1871638.5\n",
      "epoch :  12  -  cost:  1064947.8\n",
      "epoch :  13  -  cost:  921319.1\n",
      "epoch :  14  -  cost:  1450155.5\n",
      "epoch :  15  -  cost:  1463926.5\n",
      "epoch :  16  -  cost:  976299.3\n",
      "epoch :  17  -  cost:  712068.56\n",
      "epoch :  18  -  cost:  1090519.5\n",
      "epoch :  19  -  cost:  1133885.9\n",
      "epoch :  20  -  cost:  800123.25\n",
      "epoch :  21  -  cost:  693810.7\n",
      "epoch :  22  -  cost:  888877.75\n",
      "epoch :  23  -  cost:  903709.44\n",
      "epoch :  24  -  cost:  669519.94\n",
      "epoch :  25  -  cost:  611321.2\n",
      "epoch :  26  -  cost:  783214.3\n",
      "epoch :  27  -  cost:  679522.0\n",
      "epoch :  28  -  cost:  528203.56\n",
      "epoch :  29  -  cost:  623926.75\n",
      "epoch :  30  -  cost:  664216.75\n",
      "epoch :  31  -  cost:  527657.9\n",
      "epoch :  32  -  cost:  489992.94\n",
      "epoch :  33  -  cost:  589604.4\n",
      "epoch :  34  -  cost:  518972.8\n",
      "epoch :  35  -  cost:  441751.72\n",
      "epoch :  36  -  cost:  511982.0\n",
      "epoch :  37  -  cost:  493042.62\n",
      "epoch :  38  -  cost:  417473.4\n",
      "epoch :  39  -  cost:  450588.34\n",
      "epoch :  40  -  cost:  449672.62\n",
      "epoch :  41  -  cost:  389502.78\n",
      "epoch :  42  -  cost:  417481.38\n",
      "epoch :  43  -  cost:  406354.94\n",
      "epoch :  44  -  cost:  364875.72\n",
      "epoch :  45  -  cost:  389233.8\n",
      "epoch :  46  -  cost:  376101.94\n",
      "epoch :  47  -  cost:  343185.16\n",
      "epoch :  48  -  cost:  366427.9\n",
      "epoch :  49  -  cost:  344366.66\n",
      "epoch :  50  -  cost:  329204.4\n",
      "epoch :  51  -  cost:  341003.94\n",
      "epoch :  52  -  cost:  316237.88\n",
      "epoch :  53  -  cost:  314955.78\n",
      "epoch :  54  -  cost:  313605.8\n",
      "epoch :  55  -  cost:  293855.97\n",
      "epoch :  56  -  cost:  300974.22\n",
      "epoch :  57  -  cost:  285431.12\n",
      "epoch :  58  -  cost:  281009.56\n",
      "epoch :  59  -  cost:  279010.84\n",
      "epoch :  60  -  cost:  265278.7\n",
      "epoch :  61  -  cost:  267724.22\n",
      "epoch :  62  -  cost:  255052.34\n",
      "epoch :  63  -  cost:  254089.97\n",
      "epoch :  64  -  cost:  246942.61\n",
      "epoch :  65  -  cost:  241416.88\n",
      "epoch :  66  -  cost:  238507.8\n",
      "epoch :  67  -  cost:  230488.44\n",
      "epoch :  68  -  cost:  228425.66\n",
      "epoch :  69  -  cost:  221597.94\n",
      "epoch :  70  -  cost:  218914.42\n",
      "epoch :  71  -  cost:  213804.28\n",
      "epoch :  72  -  cost:  209037.55\n",
      "epoch :  73  -  cost:  204921.48\n",
      "epoch :  74  -  cost:  201296.48\n",
      "epoch :  75  -  cost:  196193.69\n",
      "epoch :  76  -  cost:  192790.33\n",
      "epoch :  77  -  cost:  188830.94\n",
      "epoch :  78  -  cost:  184719.72\n",
      "epoch :  79  -  cost:  180489.92\n",
      "epoch :  80  -  cost:  177704.28\n",
      "epoch :  81  -  cost:  172377.06\n",
      "epoch :  82  -  cost:  169777.27\n",
      "epoch :  83  -  cost:  165302.5\n",
      "epoch :  84  -  cost:  162449.06\n",
      "epoch :  85  -  cost:  158242.47\n",
      "epoch :  86  -  cost:  155385.86\n",
      "epoch :  87  -  cost:  151261.16\n",
      "epoch :  88  -  cost:  148109.72\n",
      "epoch :  89  -  cost:  144786.14\n",
      "epoch :  90  -  cost:  141695.64\n",
      "epoch :  91  -  cost:  138926.86\n",
      "epoch :  92  -  cost:  134742.23\n",
      "epoch :  93  -  cost:  132025.2\n",
      "epoch :  94  -  cost:  129707.81\n",
      "epoch :  95  -  cost:  127152.56\n",
      "epoch :  96  -  cost:  127575.58\n",
      "epoch :  97  -  cost:  123761.92\n",
      "epoch :  98  -  cost:  121598.56\n",
      "epoch :  99  -  cost:  117062.914\n",
      "epoch :  100  -  cost:  113349.68\n",
      "epoch :  101  -  cost:  109656.7\n",
      "epoch :  102  -  cost:  108629.85\n",
      "epoch :  103  -  cost:  108987.1\n",
      "epoch :  104  -  cost:  107680.234\n",
      "epoch :  105  -  cost:  103853.414\n",
      "epoch :  106  -  cost:  98935.53\n",
      "epoch :  107  -  cost:  97238.414\n",
      "epoch :  108  -  cost:  93660.95\n",
      "epoch :  109  -  cost:  94501.11\n",
      "epoch :  110  -  cost:  93989.3\n",
      "epoch :  111  -  cost:  92086.26\n",
      "epoch :  112  -  cost:  89029.09\n",
      "epoch :  113  -  cost:  84093.96\n",
      "epoch :  114  -  cost:  82696.266\n",
      "epoch :  115  -  cost:  80178.71\n",
      "epoch :  116  -  cost:  79831.375\n",
      "epoch :  117  -  cost:  77858.04\n",
      "epoch :  118  -  cost:  77090.34\n",
      "epoch :  119  -  cost:  74362.93\n",
      "epoch :  120  -  cost:  73629.18\n",
      "epoch :  121  -  cost:  72209.266\n",
      "epoch :  122  -  cost:  71659.91\n",
      "epoch :  123  -  cost:  73634.59\n",
      "epoch :  124  -  cost:  79953.34\n",
      "epoch :  125  -  cost:  77064.086\n",
      "epoch :  126  -  cost:  72598.28\n",
      "epoch :  127  -  cost:  64865.914\n",
      "epoch :  128  -  cost:  60841.87\n",
      "epoch :  129  -  cost:  60482.06\n",
      "epoch :  130  -  cost:  64621.176\n",
      "epoch :  131  -  cost:  72116.125\n",
      "epoch :  132  -  cost:  62820.184\n",
      "epoch :  133  -  cost:  55412.008\n",
      "epoch :  134  -  cost:  55341.535\n",
      "epoch :  135  -  cost:  59155.586\n",
      "epoch :  136  -  cost:  53471.586\n",
      "epoch :  137  -  cost:  49645.523\n",
      "epoch :  138  -  cost:  50224.94\n",
      "epoch :  139  -  cost:  48308.645\n",
      "epoch :  140  -  cost:  46425.57\n",
      "epoch :  141  -  cost:  45709.723\n",
      "epoch :  142  -  cost:  44856.848\n",
      "epoch :  143  -  cost:  42193.832\n",
      "epoch :  144  -  cost:  42741.477\n",
      "epoch :  145  -  cost:  40475.46\n",
      "epoch :  146  -  cost:  40547.68\n",
      "epoch :  147  -  cost:  38393.953\n",
      "epoch :  148  -  cost:  38711.676\n",
      "epoch :  149  -  cost:  35902.543\n",
      "epoch :  150  -  cost:  36066.33\n",
      "epoch :  151  -  cost:  34552.164\n",
      "epoch :  152  -  cost:  36210.72\n",
      "epoch :  153  -  cost:  37756.26\n",
      "epoch :  154  -  cost:  43771.168\n",
      "epoch :  155  -  cost:  59499.223\n",
      "epoch :  156  -  cost:  55368.066\n",
      "epoch :  157  -  cost:  36353.51\n",
      "epoch :  158  -  cost:  29116.727\n",
      "epoch :  159  -  cost:  29758.955\n",
      "epoch :  160  -  cost:  34631.934\n",
      "epoch :  161  -  cost:  35367.938\n",
      "epoch :  162  -  cost:  33476.152\n",
      "epoch :  163  -  cost:  27181.656\n",
      "epoch :  164  -  cost:  25387.154\n",
      "epoch :  165  -  cost:  25776.719\n",
      "epoch :  166  -  cost:  23814.068\n",
      "epoch :  167  -  cost:  24426.605\n",
      "epoch :  168  -  cost:  24361.523\n",
      "epoch :  169  -  cost:  21849.69\n",
      "epoch :  170  -  cost:  23361.283\n",
      "epoch :  171  -  cost:  25012.758\n",
      "epoch :  172  -  cost:  21869.307\n",
      "epoch :  173  -  cost:  19971.951\n",
      "epoch :  174  -  cost:  20255.82\n",
      "epoch :  175  -  cost:  19203.814\n",
      "epoch :  176  -  cost:  18293.484\n",
      "epoch :  177  -  cost:  18371.383\n",
      "epoch :  178  -  cost:  16967.709\n",
      "epoch :  179  -  cost:  17572.75\n",
      "epoch :  180  -  cost:  17497.902\n",
      "epoch :  181  -  cost:  16208.4\n",
      "epoch :  182  -  cost:  15254.104\n",
      "epoch :  183  -  cost:  15317.14\n",
      "epoch :  184  -  cost:  14308.16\n",
      "epoch :  185  -  cost:  14461.644\n",
      "epoch :  186  -  cost:  14946.835\n",
      "epoch :  187  -  cost:  16595.855\n",
      "epoch :  188  -  cost:  21765.05\n",
      "epoch :  189  -  cost:  37080.008\n",
      "epoch :  190  -  cost:  60210.08\n",
      "epoch :  191  -  cost:  64207.246\n",
      "epoch :  192  -  cost:  24456.693\n",
      "epoch :  193  -  cost:  13330.964\n",
      "epoch :  194  -  cost:  23027.67\n",
      "epoch :  195  -  cost:  46327.703\n",
      "epoch :  196  -  cost:  42355.816\n",
      "epoch :  197  -  cost:  16828.875\n",
      "epoch :  198  -  cost:  20797.996\n",
      "epoch :  199  -  cost:  46125.14\n",
      "epoch :  200  -  cost:  24184.86\n",
      "epoch :  201  -  cost:  12708.025\n",
      "epoch :  202  -  cost:  30257.375\n",
      "epoch :  203  -  cost:  26464.646\n",
      "epoch :  204  -  cost:  21953.799\n",
      "epoch :  205  -  cost:  13091.884\n",
      "epoch :  206  -  cost:  22454.033\n",
      "epoch :  207  -  cost:  13762.645\n",
      "epoch :  208  -  cost:  14652.549\n",
      "epoch :  209  -  cost:  14367.29\n",
      "epoch :  210  -  cost:  10792.193\n",
      "epoch :  211  -  cost:  17460.373\n",
      "epoch :  212  -  cost:  9254.597\n",
      "epoch :  213  -  cost:  12050.883\n",
      "epoch :  214  -  cost:  9039.018\n",
      "epoch :  215  -  cost:  8862.239\n",
      "epoch :  216  -  cost:  9151.779\n",
      "epoch :  217  -  cost:  8796.334\n",
      "epoch :  218  -  cost:  8365.757\n",
      "epoch :  219  -  cost:  7235.433\n",
      "epoch :  220  -  cost:  8008.038\n",
      "epoch :  221  -  cost:  7192.1704\n",
      "epoch :  222  -  cost:  7100.497\n",
      "epoch :  223  -  cost:  6343.068\n",
      "epoch :  224  -  cost:  6842.403\n",
      "epoch :  225  -  cost:  6103.0225\n",
      "epoch :  226  -  cost:  6491.4346\n",
      "epoch :  227  -  cost:  5626.1895\n",
      "epoch :  228  -  cost:  5756.047\n",
      "epoch :  229  -  cost:  5868.533\n",
      "epoch :  230  -  cost:  5300.144\n",
      "epoch :  231  -  cost:  5157.806\n",
      "epoch :  232  -  cost:  4412.3755\n",
      "epoch :  233  -  cost:  4749.6987\n",
      "epoch :  234  -  cost:  4202.3687\n",
      "epoch :  235  -  cost:  4438.655\n",
      "epoch :  236  -  cost:  3690.08\n",
      "epoch :  237  -  cost:  4010.2905\n",
      "epoch :  238  -  cost:  3491.6812\n",
      "epoch :  239  -  cost:  3570.0667\n",
      "epoch :  240  -  cost:  3195.3452\n",
      "epoch :  241  -  cost:  3382.4106\n",
      "epoch :  242  -  cost:  3421.4329\n",
      "epoch :  243  -  cost:  2773.3242\n",
      "epoch :  244  -  cost:  3165.3035\n",
      "epoch :  245  -  cost:  3368.2366\n",
      "epoch :  246  -  cost:  2540.062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  247  -  cost:  2851.7358\n",
      "epoch :  248  -  cost:  2849.8657\n",
      "epoch :  249  -  cost:  2281.6748\n",
      "epoch :  250  -  cost:  2361.5015\n",
      "epoch :  251  -  cost:  2266.232\n",
      "epoch :  252  -  cost:  2049.276\n",
      "epoch :  253  -  cost:  2311.0032\n",
      "epoch :  254  -  cost:  2083.2883\n",
      "epoch :  255  -  cost:  1901.2051\n",
      "epoch :  256  -  cost:  1661.66\n",
      "epoch :  257  -  cost:  1811.2499\n",
      "epoch :  258  -  cost:  1585.5221\n",
      "epoch :  259  -  cost:  2050.605\n",
      "epoch :  260  -  cost:  2707.3684\n",
      "epoch :  261  -  cost:  2528.0715\n",
      "epoch :  262  -  cost:  2034.7919\n",
      "epoch :  263  -  cost:  1337.309\n",
      "epoch :  264  -  cost:  1793.634\n",
      "epoch :  265  -  cost:  1254.75\n",
      "epoch :  266  -  cost:  2206.135\n",
      "epoch :  267  -  cost:  3090.0247\n",
      "epoch :  268  -  cost:  1216.102\n",
      "epoch :  269  -  cost:  5275.4443\n",
      "epoch :  270  -  cost:  22383.814\n",
      "epoch :  271  -  cost:  57909.98\n",
      "epoch :  272  -  cost:  29257.18\n",
      "epoch :  273  -  cost:  6414.3916\n",
      "epoch :  274  -  cost:  1808.4667\n",
      "epoch :  275  -  cost:  3749.1038\n",
      "epoch :  276  -  cost:  1785.8789\n",
      "epoch :  277  -  cost:  2196.4062\n",
      "epoch :  278  -  cost:  1592.6084\n",
      "epoch :  279  -  cost:  1554.9387\n",
      "epoch :  280  -  cost:  2043.1512\n",
      "epoch :  281  -  cost:  1240.9076\n",
      "epoch :  282  -  cost:  1628.5575\n",
      "epoch :  283  -  cost:  1154.5039\n",
      "epoch :  284  -  cost:  1137.4778\n",
      "epoch :  285  -  cost:  1350.3851\n",
      "epoch :  286  -  cost:  965.09906\n",
      "epoch :  287  -  cost:  924.2809\n",
      "epoch :  288  -  cost:  1167.4883\n",
      "epoch :  289  -  cost:  806.7199\n",
      "epoch :  290  -  cost:  1086.7888\n",
      "epoch :  291  -  cost:  793.03406\n",
      "epoch :  292  -  cost:  693.1794\n",
      "epoch :  293  -  cost:  816.0487\n",
      "epoch :  294  -  cost:  651.2131\n",
      "epoch :  295  -  cost:  554.4099\n",
      "epoch :  296  -  cost:  606.4432\n",
      "epoch :  297  -  cost:  585.95087\n",
      "epoch :  298  -  cost:  450.43887\n",
      "epoch :  299  -  cost:  430.36334\n",
      "epoch :  300  -  cost:  504.11755\n",
      "epoch :  301  -  cost:  358.32422\n",
      "epoch :  302  -  cost:  336.5072\n",
      "epoch :  303  -  cost:  362.32816\n",
      "epoch :  304  -  cost:  316.61133\n",
      "epoch :  305  -  cost:  254.19562\n",
      "epoch :  306  -  cost:  214.42728\n",
      "epoch :  307  -  cost:  261.87518\n",
      "epoch :  308  -  cost:  179.02939\n",
      "epoch :  309  -  cost:  179.03088\n",
      "epoch :  310  -  cost:  178.83185\n",
      "epoch :  311  -  cost:  167.83553\n",
      "epoch :  312  -  cost:  151.39763\n",
      "epoch :  313  -  cost:  130.65369\n",
      "epoch :  314  -  cost:  110.98473\n",
      "epoch :  315  -  cost:  135.24132\n",
      "epoch :  316  -  cost:  98.70017\n",
      "epoch :  317  -  cost:  90.54623\n",
      "epoch :  318  -  cost:  94.27421\n",
      "epoch :  319  -  cost:  95.23176\n",
      "epoch :  320  -  cost:  80.367195\n",
      "epoch :  321  -  cost:  71.9664\n",
      "epoch :  322  -  cost:  62.920174\n",
      "epoch :  323  -  cost:  56.04956\n",
      "epoch :  324  -  cost:  57.09\n",
      "epoch :  325  -  cost:  49.874123\n",
      "epoch :  326  -  cost:  50.893246\n",
      "epoch :  327  -  cost:  50.20123\n",
      "epoch :  328  -  cost:  47.978157\n",
      "epoch :  329  -  cost:  39.947544\n",
      "epoch :  330  -  cost:  32.140087\n",
      "epoch :  331  -  cost:  23.45307\n",
      "epoch :  332  -  cost:  13.999561\n",
      "epoch :  333  -  cost:  18.021755\n",
      "epoch :  334  -  cost:  2.8132455\n",
      "epoch :  335  -  cost:  2.6586843\n",
      "epoch :  336  -  cost:  3.6421053\n",
      "epoch :  337  -  cost:  8.08193\n",
      "epoch :  338  -  cost:  2.8157017\n",
      "epoch :  339  -  cost:  4.3928947\n",
      "epoch :  340  -  cost:  0.0\n",
      "epoch :  341  -  cost:  0.0\n",
      "epoch :  342  -  cost:  0.092982456\n",
      "epoch :  343  -  cost:  0.0\n",
      "epoch :  344  -  cost:  0.0\n",
      "epoch :  345  -  cost:  0.0\n",
      "epoch :  346  -  cost:  0.0\n",
      "epoch :  347  -  cost:  0.0\n",
      "epoch :  348  -  cost:  0.0\n",
      "epoch :  349  -  cost:  0.0\n",
      "epoch :  350  -  cost:  0.0\n",
      "epoch :  351  -  cost:  0.67\n",
      "epoch :  352  -  cost:  0.0\n",
      "epoch :  353  -  cost:  0.0\n",
      "epoch :  354  -  cost:  0.0\n",
      "epoch :  355  -  cost:  0.0\n",
      "epoch :  356  -  cost:  0.0\n",
      "epoch :  357  -  cost:  0.0\n",
      "epoch :  358  -  cost:  0.0\n",
      "epoch :  359  -  cost:  0.0\n",
      "epoch :  360  -  cost:  0.0\n",
      "epoch :  361  -  cost:  0.0\n",
      "epoch :  362  -  cost:  0.0\n",
      "epoch :  363  -  cost:  0.11008772\n",
      "epoch :  364  -  cost:  0.0\n",
      "epoch :  365  -  cost:  0.0\n",
      "epoch :  366  -  cost:  0.0\n",
      "epoch :  367  -  cost:  0.0\n",
      "epoch :  368  -  cost:  0.0\n",
      "epoch :  369  -  cost:  0.0\n",
      "epoch :  370  -  cost:  0.0\n",
      "epoch :  371  -  cost:  0.0\n",
      "epoch :  372  -  cost:  0.0\n",
      "epoch :  373  -  cost:  0.539386\n",
      "epoch :  374  -  cost:  0.0\n",
      "epoch :  375  -  cost:  0.0\n",
      "epoch :  376  -  cost:  0.0\n",
      "epoch :  377  -  cost:  0.0\n",
      "epoch :  378  -  cost:  0.0\n",
      "epoch :  379  -  cost:  0.0\n",
      "epoch :  380  -  cost:  0.0\n",
      "epoch :  381  -  cost:  0.0\n",
      "epoch :  382  -  cost:  0.0\n",
      "epoch :  383  -  cost:  0.0\n",
      "epoch :  384  -  cost:  0.0\n",
      "epoch :  385  -  cost:  0.0\n",
      "epoch :  386  -  cost:  0.0\n",
      "epoch :  387  -  cost:  0.0\n",
      "epoch :  388  -  cost:  0.0\n",
      "epoch :  389  -  cost:  0.0\n",
      "epoch :  390  -  cost:  0.0\n",
      "epoch :  391  -  cost:  0.0\n",
      "epoch :  392  -  cost:  0.0\n",
      "epoch :  393  -  cost:  0.0\n",
      "epoch :  394  -  cost:  0.0\n",
      "epoch :  395  -  cost:  0.0\n",
      "epoch :  396  -  cost:  0.0\n",
      "epoch :  397  -  cost:  0.0\n",
      "epoch :  398  -  cost:  0.0\n",
      "epoch :  399  -  cost:  0.0\n",
      "epoch :  400  -  cost:  0.0\n",
      "epoch :  401  -  cost:  0.0\n",
      "epoch :  402  -  cost:  0.0\n",
      "epoch :  403  -  cost:  0.0\n",
      "epoch :  404  -  cost:  0.0\n",
      "epoch :  405  -  cost:  0.0\n",
      "epoch :  406  -  cost:  0.0\n",
      "epoch :  407  -  cost:  0.0\n",
      "epoch :  408  -  cost:  0.0\n",
      "epoch :  409  -  cost:  0.0\n",
      "epoch :  410  -  cost:  0.0\n",
      "epoch :  411  -  cost:  0.0\n",
      "epoch :  412  -  cost:  0.0\n",
      "epoch :  413  -  cost:  0.0\n",
      "epoch :  414  -  cost:  0.0\n",
      "epoch :  415  -  cost:  0.0\n",
      "epoch :  416  -  cost:  0.0\n",
      "epoch :  417  -  cost:  0.0\n",
      "epoch :  418  -  cost:  0.0\n",
      "epoch :  419  -  cost:  0.0\n",
      "epoch :  420  -  cost:  0.0\n",
      "epoch :  421  -  cost:  0.0\n",
      "epoch :  422  -  cost:  0.0\n",
      "epoch :  423  -  cost:  0.0\n",
      "epoch :  424  -  cost:  0.0\n",
      "epoch :  425  -  cost:  0.0\n",
      "epoch :  426  -  cost:  0.0\n",
      "epoch :  427  -  cost:  0.0\n",
      "epoch :  428  -  cost:  0.0\n",
      "epoch :  429  -  cost:  0.0\n",
      "epoch :  430  -  cost:  0.0\n",
      "epoch :  431  -  cost:  0.0\n",
      "epoch :  432  -  cost:  0.0\n",
      "epoch :  433  -  cost:  0.0\n",
      "epoch :  434  -  cost:  0.0\n",
      "epoch :  435  -  cost:  0.0\n",
      "epoch :  436  -  cost:  0.0\n",
      "epoch :  437  -  cost:  0.0\n",
      "epoch :  438  -  cost:  0.0\n",
      "epoch :  439  -  cost:  0.0\n",
      "epoch :  440  -  cost:  0.0\n",
      "epoch :  441  -  cost:  0.0\n",
      "epoch :  442  -  cost:  0.0\n",
      "epoch :  443  -  cost:  0.0\n",
      "epoch :  444  -  cost:  0.0\n",
      "epoch :  445  -  cost:  0.0\n",
      "epoch :  446  -  cost:  0.0\n",
      "epoch :  447  -  cost:  0.0\n",
      "epoch :  448  -  cost:  0.0\n",
      "epoch :  449  -  cost:  0.0\n",
      "epoch :  450  -  cost:  0.0\n",
      "epoch :  451  -  cost:  0.0\n",
      "epoch :  452  -  cost:  0.0\n",
      "epoch :  453  -  cost:  0.0\n",
      "epoch :  454  -  cost:  0.0\n",
      "epoch :  455  -  cost:  0.0\n",
      "epoch :  456  -  cost:  0.0\n",
      "epoch :  457  -  cost:  0.0\n",
      "epoch :  458  -  cost:  0.0\n",
      "epoch :  459  -  cost:  0.0\n",
      "epoch :  460  -  cost:  0.0\n",
      "epoch :  461  -  cost:  0.0\n",
      "epoch :  462  -  cost:  0.0\n",
      "epoch :  463  -  cost:  0.0\n",
      "epoch :  464  -  cost:  0.0\n",
      "epoch :  465  -  cost:  0.0\n",
      "epoch :  466  -  cost:  0.0\n",
      "epoch :  467  -  cost:  0.0\n",
      "epoch :  468  -  cost:  0.0\n",
      "epoch :  469  -  cost:  0.0\n",
      "epoch :  470  -  cost:  0.0\n",
      "epoch :  471  -  cost:  0.0\n",
      "epoch :  472  -  cost:  0.0\n",
      "epoch :  473  -  cost:  0.0\n",
      "epoch :  474  -  cost:  0.0\n",
      "epoch :  475  -  cost:  0.0\n",
      "epoch :  476  -  cost:  0.0\n",
      "epoch :  477  -  cost:  0.0\n",
      "epoch :  478  -  cost:  0.0\n",
      "epoch :  479  -  cost:  0.0\n",
      "epoch :  480  -  cost:  0.0\n",
      "epoch :  481  -  cost:  0.0\n",
      "epoch :  482  -  cost:  0.0\n",
      "epoch :  483  -  cost:  0.0\n",
      "epoch :  484  -  cost:  0.0\n",
      "epoch :  485  -  cost:  0.0\n",
      "epoch :  486  -  cost:  0.0\n",
      "epoch :  487  -  cost:  0.0\n",
      "epoch :  488  -  cost:  0.0\n",
      "epoch :  489  -  cost:  0.0\n",
      "epoch :  490  -  cost:  0.0\n",
      "epoch :  491  -  cost:  0.0\n",
      "epoch :  492  -  cost:  0.0\n",
      "epoch :  493  -  cost:  0.0\n",
      "epoch :  494  -  cost:  0.0\n",
      "epoch :  495  -  cost:  0.0\n",
      "epoch :  496  -  cost:  0.0\n",
      "epoch :  497  -  cost:  0.0\n",
      "epoch :  498  -  cost:  0.0\n",
      "epoch :  499  -  cost:  0.0\n"
     ]
    }
   ],
   "source": [
    "cost_history=[]\n",
    "n_epochs =500\n",
    "\n",
    "sess = tf.compat.v1.Session()\n",
    "sess.run(init)\n",
    "train_y=train_y.todense()\n",
    "for i in range(n_epochs):\n",
    "    a, c = sess.run([optimizer, cost], feed_dict={x: train_x, y_: train_y})  \n",
    "    cost_history = np.append(cost_history,c)  \n",
    "    print('epoch : ', i, ' - ', 'cost: ', c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5650acf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(range(0,500), cost_history[0:500])\n",
    "plt.ylabel('COST')\n",
    "plt.xlabel('EPOCH')\n",
    "plt.ylim([0, 1000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16f56cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.62\n"
     ]
    }
   ],
   "source": [
    "test_y=test_y.todense() \n",
    "correct_prediction = tf.equal(tf.argmax(model,1), tf.argmax(y_,1))   \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={x: test_x, y_:test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c619270a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf1] *",
   "language": "python",
   "name": "conda-env-tf1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
